from src.scrap import scrape_site
from src.utils.utils_io import save_json
from src.utils.utils_clean import normalize_articles
from datetime import date
from pathlib import Path
import json
from src.traitement import main as traitement_main
from src.newsletter_sections import main as newsletter_main

# --- Liste des sites Ã  scraper ---
SITES = [
    ("https://news.artnet.com/art-world/science-technology/feed", "Artnet"),
    ("https://www.artnews.com/c/art-news/news/feed/", "ArtNews"),
    ("https://news.artnet.com/market","Artnet market"),
    ("https://news.artnet.com/multimedia","Artnet multimedia"),
    ("https://techcrunch.com/category/artificial-intelligence/", "TechCrunch"),
    ("https://fr.artprice.com/artprice-news","Artprice"),
    ("https://fr.artprice.com/artmarketinsight","ArtPrice ArtMarketInsight"),
    ("https://fr.cointelegraph.com/tags/nft","Cointelegraph NFT"),
    ("https://fr.cointelegraph.com/tags/ai","Cointelegraph AI"),
    ("https://news.artnet.com/multimedia","Dezeen Technology"),
    ("https://www.fastcompany.com/technology","Fastcompany Tech"),
    ("https://www.engadget.com/ai/","Engadget AI"),
    ("https://www.engadget.com/science/robotics/","Engadget robotics")
]


def canonical_url(u: str) -> str:
    """Mini normalisation locale (sans dÃ©pendance)"""
    if not u: return ""
    u = u.strip()
    if u.endswith("/"): u = u[:-1]
    return u.lower()

def load_processed_urls(base_dir="data/processed") -> set[str]:
    """Lit toutes les URLs dÃ©jÃ  traitÃ©es dans processed/**/articles.json"""
    seen = set()
    root = Path(base_dir)
    if not root.exists():
        return seen
    for f in root.glob("**/articles.json"):
        try:
            data = json.loads(f.read_text(encoding="utf-8"))
            for it in data or []:
                cu = canonical_url((it or {}).get("url", ""))
                if cu:
                    seen.add(cu)
        except Exception:
            continue
    return seen

print("ğŸš€ Lancement du scraping multi-sources...\n")

# 0) URLs dÃ©jÃ  traitÃ©es
already_done = load_processed_urls()
print(f"ğŸ§  URLs dÃ©jÃ  traitÃ©es trouvÃ©es : {len(already_done)}\n")

# 1) Scrape tout (on ne modifie pas scrap.py)
all_articles = []
for url, src in SITES:
    print(f"ğŸ“° Scraping {src} ...")
    try:
        arts = scrape_site(url, source=src, limit=5)
        all_articles += arts
        print(f"âœ… {len(arts)} articles rÃ©cupÃ©rÃ©s depuis {src}\n")
    except Exception as e:
        print(f"âš ï¸ Erreur sur {src} : {e}\n")

# 2) Filtrer ce qui est dÃ©jÃ  traitÃ© (post-filtre ultra simple)
new_articles = []
for a in all_articles:
    cu = canonical_url(a.get("url", ""))
    if cu and cu not in already_done:
        new_articles.append(a)

print(f"ğŸ†• Nouveaux articles aprÃ¨s filtre : {len(new_articles)}")

# 3) Sauvegarde brute
save_json(new_articles)

# 4) Nettoyage et sauvegarde "processed"
cleaned = normalize_articles(new_articles)
processed_dir = Path("data/processed") / date.today().strftime("%d-%m-%Y")
processed_dir.mkdir(parents=True, exist_ok=True)
processed_path = processed_dir / "articles.json"
with open(processed_path, "w", encoding="utf-8") as f:
    json.dump(cleaned, f, ensure_ascii=False, indent=2)

print(f"âœ… Nettoyage terminÃ© â†’ {processed_path}")
print(f"ğŸ§¾ Total final (nouveaux uniques) : {len(cleaned)}")

print("\nğŸ§  Ã‰tape suivante : gÃ©nÃ©ration des rÃ©sumÃ©s...")
traitement_main()

print("\nğŸ“° Ã‰tape suivante : crÃ©ation de la newsletter...")
newsletter_main()

print("\nâœ… Pipeline complet terminÃ© ! Newsletter prÃªte dans newsletter.html")